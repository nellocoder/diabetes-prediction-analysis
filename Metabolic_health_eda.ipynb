{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febea3c5-c473-4219-9b36-befa8a42856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the working directory to our target directory\n",
    "import os\n",
    "os.chdir(r\"C:\\Users\\User\\OneDrive\\Documents\\Data Science 2025.2026\")\n",
    "os.getcwd()#will make us view the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b686e-94c3-48f1-af43-a240e401c07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas (to install pandas if not already installed)\n",
    "# You need to first install all the packages before importing them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd6841-d16e-4e11-8b84-26ae67ec2004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044316c4-c6d1-4a65-a623-dbd5d973f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing our data. NB This dataset was obtained from Kaggle. \n",
    "insulin_data=pd.read_csv(\"insulin_dosage_prediction.csv\")\n",
    "df=insulin_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c3e93-8bce-4f5d-b40e-582e11caf64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the first 5 rows of the data set\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca90b35-a7e0-4800-ba8a-dbf5ee0d7eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())#check for missing values or data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce371e-d944-42b4-bd34-2e69546fc36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the data summaries and see if we have any outliers\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74df8511-2c88-4c1a-bc52-2d77b2ec41ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets create a correlation heat map (Look for squares that are dark red\n",
    "#strong positive correlation) or dark blue (strong negative correlation)\n",
    "#Also note that we only need to run correlation for numeric values, therefore we need to drop all the non-numeric values\n",
    "numerical_data = df.select_dtypes(include=['float64', 'int64'])\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(numerical_data.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap: How variables relate to each other\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a162f7fc-53c6-4ad7-a14f-ab334a077e89",
   "metadata": {},
   "source": [
    "These correlations indicate that the data is either random or synthetic. Studies have indicated that there is a strong correlation between BMI and weight. Furthermore, there should be a high correlation between HbA1c and glucose level, but our data shows a zero correlation coefficient, which indicates the data is actually not correct (randomly generated). To visualize this, let us use a scatter plot and observe if we see a random effect of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088be044-38e2-4fc4-b540-facfb4f5593e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Plot 1: Weight vs BMI (Should be strongly correlated)\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x=df['weight'], y=df['BMI'])\n",
    "plt.title(\"Weight vs. BMI\")\n",
    "\n",
    "# Plot 2: Glucose vs HbA1c (Should be strongly correlated)\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x=df['glucose_level'], y=df['HbA1c'])\n",
    "plt.title(\"Glucose vs. HbA1c\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d437354-b573-45ed-a9f2-7b9bbbf75854",
   "metadata": {},
   "source": [
    "What you are seeing in this dataset is the **\"Square of Doom\"** in data analysis, which confirms that the dataset is indeed a fake one. Therefore, this data set is a fake one. We need to change the dataset for us to continue with our data science project (machine learning). Let's acquire a new data set, which we will obtain from GitHub (an Indian dataset).  See below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c2d76-cf32-4726-bb5a-fbeb08fd564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the REAL Pima Indians Diabetes dataset directly from a URL\n",
    "url=\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "column_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "new_data = pd.read_csv(url, names=column_names)\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0461f729-f10a-417d-8f34-91708be05684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Check the correlation again\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(new_data.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap: How variables relate to each other\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c31f761-2eba-4de5-aa77-b4bc5e704262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. The \"Eye Test\" - Glucose vs Insulin, the scatter plots \n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.scatterplot(x=new_data['Glucose'], y=new_data['Insulin'])\n",
    "plt.title(\"Glucose vs. Insulin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3c5de-226d-4b91-97dd-00eb4f12ff5f",
   "metadata": {},
   "source": [
    "Now, looking at the two plots (the correlation matrix and the scatter plots), we can clearly see that we are working with a real dataset. We can use this dataset to continue with our project. But first, let us do data cleaning. Let's use **describe()** and the **info()** functions to quickly assess the distribution and check for outliers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171995ca-c32d-4af7-9d1f-526013df70fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff83752-080c-4ae7-ae47-f4ccaf3eb453",
   "metadata": {},
   "source": [
    "We can see that the **info()** shows no missing values. Now what about the **describe()** function? Let us check it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3cc708-c0d7-427e-bc57-3daa947e879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4b1a3c-ff16-4273-87cf-8f17ecfd4fe0",
   "metadata": {},
   "source": [
    "Looking at the summary statistics of our dataset, we can see that we have anomalies. On the minimum values, blood pressure and BMI are actually 0, which is not possible. Nobody can have a blood pressure of zero and still be alive. We can therefore say that the ***0*** might have been used as a placeholder for a missing value. Calculating averages with such values will give us wrong values. We therefore need to fix these issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a56dc-5def-4991-82bc-34919e3a87a8",
   "metadata": {},
   "source": [
    "### Let us now remove the \"fake zeros\" and generate our first professional health dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb8d547-75c3-4208-9335-e8dbc39e2a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#DATA CLEANING\n",
    "# Replace 0 with NaN (Not a Number) in columns where 0 is impossible\n",
    "columns_to_fix = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "new_data[columns_to_fix] = new_data[columns_to_fix].replace(0, np.nan)#from the new_data\n",
    "# Print how much data was actually missing\n",
    "print(\"Missing values per column:\\n\", new_data.isnull().sum())\n",
    "# Fill missing values with the 'Median' (middle value) of that column\n",
    "# This is better than 'Mean' because it's not affected by outliers\n",
    "for col in columns_to_fix:\n",
    "    new_data[col] = new_data[col].fillna(new_data[col].median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749c3d73-aec0-4511-a437-b0ad99911e0b",
   "metadata": {},
   "source": [
    "We can see that we had 5 missing values on glucose, 35 on BP, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922cf531-a3c9-4fde-b6ac-8fe11b75d72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 1: Glucose Distribution (Healthy vs Diabetic)\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data=new_data, x='Glucose', hue='Outcome', kde=True, element=\"step\", palette='seismic')\n",
    "plt.title(\"Glucose Levels: Healthy (0) vs Diabetic (1)\", fontsize=10)\n",
    "plt.xlabel(\"Glucose Level\")\n",
    "\n",
    "# Chart 2: BMI vs Age (Who is most at risk?)\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x=new_data['Age'], y=new_data['BMI'], hue=new_data['Outcome'], palette='seismic', alpha=0.7)\n",
    "plt.title(\"BMI vs Age colored by Diabetes Outcome\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a5fb09-ba16-48d4-b4b8-35a70b59e1a0",
   "metadata": {},
   "source": [
    "â€¢ **data=new** tells seaborn to use the dataframe named new.\\\n",
    "â€¢ **x='Glucose'** selects the Glucose column for the horizontal axis.\\\n",
    "â€¢ **hue='Outcome'** splits the plot by the Outcome column so each class has a different color.\\\n",
    "â€¢ **kde=True** adds a smooth density curve on top of the histogram.\\\n",
    "â€¢ **element=\"step\"** draws the histogram with outlined steps instead of filled bars.\\\n",
    "â€¢ **palette='seismic'** uses the seismic color palette to color the different Outcome groups.\\\n",
    "1. You can see that we have \"mountains\" on the first chart. The Red mountain (diabetic) should be shifted to the right (higher glucose) compared to the Blue mountain.\n",
    "2. On chart, look for patterns. The red dots (Diabetics) mostly at the top (High BMI). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502ad1b-0c2b-4846-943e-75fa1e5e4710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "547a0257-af6e-46f3-9675-a2a401de2cab",
   "metadata": {},
   "source": [
    "# Building Predictive Models\n",
    "Now lets dive deep into building some models. Yaaaaaaay! Are you ready for this? I am all ready!!\n",
    "## 1. Insulin Predictor\n",
    "Even in the real world, insulin treatment costs a lot of money. Glucose and BMI measurement is cheap and easy. From this dataset, can we build a model that predicts a patient's insulin level by just looking at the BMI, glucose, and age? \\\n",
    "From our data cleaning exercise, you remember that we cleaned the dataset by using the median to replace the missing values, but in **Machine Learning (ML)**, that's actually cheating!! (funny, right? ðŸ˜€). We therefore need to use the original dataset (the raw data), and only use the rows in which we know the insulin level.\\\n",
    "But first, let us import some libraries.\\\n",
    "**We will be using the scikit-learn library.\n",
    "It will do the following:**\\\n",
    "i. It will filter out bad data\\\n",
    "ii. Split the data: 80% (training data) and 20% (test data)\\\n",
    "iii. Train a linear regression model.\\\n",
    "iv. Test how accurate the model is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7429c0-c120-4335-8cd2-49ca490b722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c595803d-7dfd-4ef9-9cf7-cc12a8dfb5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9073a4-7db1-4477-99d9-0cf17144ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LOAD & FILTER\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "col_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DPF', 'Age', 'Outcome']\n",
    "df = pd.read_csv(url, names=col_names)\n",
    "\n",
    "# We only want rows where Insulin is NOT 0 (Real data only)\n",
    "# We also need Glucose and BMI to be real (not 0)\n",
    "clean_data = df[(df['Insulin'] != 0) & (df['BMI'] != 0) & (df['Glucose'] != 0)]#extracts the non zero values of Insulin, BMI and Glucose\n",
    "\n",
    "# 2. DEFINE X (The inputs) AND y (The target)\n",
    "# We will try to predict 'Insulin' using Glucose, BMI, and Age\n",
    "X = clean_data[['Glucose', 'BMI', 'Age']]#sets these variables as the independent variable (inputs)\n",
    "y = clean_data['Insulin']# This will be our dependent variable (our outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd4f80-b908-4cae-b26a-2ae54d255220",
   "metadata": {},
   "source": [
    "Since we have loaded our data successfully, and identified the inputs and outputs for our model, we can now go ahead and split the data into two; Train data and test data (the 80-20). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe12301-ce43-42b1-92af-9cc8a76e87c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. SPLIT DATA (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39395ab-a55d-4009-8d90-ec74a7f38bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. TRAIN THE MODEL\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba14238-e0a4-467f-aa04-e2a626e31db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. We now EVALUATE the model and see how best it performs.\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(\"--- Model Performance ---\")\n",
    "print(f\"R-Squared Score: {r2_score(y_test, predictions):.2f} (Closer to 1.0 is better)\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, predictions):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3451ac9-6b1d-439d-9965-3e95a5103eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Let us try to visualize the prediction against what we really know\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.scatterplot(x=y_test, y=predictions, alpha=0.6)\n",
    "plt.plot([0, 600], [0, 600], color='red', linestyle='--') # Perfect prediction line\n",
    "plt.xlabel(\"Actual Insulin Levels\")\n",
    "plt.ylabel(\"Predicted Insulin Levels\")\n",
    "plt.title(\"Actual vs Predicted Insulin (Linear Regression)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b1922c-e538-4819-b804-556ed9eda6af",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "An R-squared of **0.41** means the model explains about 41% of the variation in insulin levels. \\\n",
    "The MAE of **60.85** is high. Practically, this means that a patient's real insulin is 100; your model might guess 160 or 40. That is big enough of a gap to be dangerous in a medical setting. This actually means that the linear regression model has failed. We there need to try the random forest to correct this phenomenon. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1640886f-cf29-4a97-8c07-ef2dbaadba1e",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a449805-ee93-4f16-bd48-59bee57f71e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# 1. SETUP THE NEW MODEL\n",
    "# n_estimators=100 means \"build 100 decision trees\"\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# 2. TRAIN\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 3. PREDICT\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# 4. COMPARE RESULTS\n",
    "print(\"--- Model Battle: Linear vs. Random Forest ---\")\n",
    "print(f\"Linear Regression R2:  0.41\") # Your previous score\n",
    "print(f\"Random Forest R2:      {r2_score(y_test, rf_predictions):.2f}\")\n",
    "print(f\"Random Forest Error:   {mean_absolute_error(y_test, rf_predictions):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dd3f3d-2157-4376-bf14-79378ee8d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. VISUALIZE THE IMPROVEMENT\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Linear Regression (The old one)\n",
    "plt.subplot(1, 2, 1)\n",
    "# Note: 'predictions' variable comes from your previous linear regression run\n",
    "# If you lost it, re-run the linear regression code first.\n",
    "sns.scatterplot(x=y_test, y=predictions, alpha=0.4, color='blue')\n",
    "plt.plot([0, 600], [0, 600], '--r') # Perfect prediction line\n",
    "plt.xlabel(\"Actual Insulin\")\n",
    "plt.ylabel(\"Predicted Insulin\")\n",
    "plt.title(\"Linear Regression (Old)\")\n",
    "\n",
    "# Plot 2: Random Forest (The new one)\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x=y_test, y=rf_predictions, alpha=0.4, color='green')\n",
    "plt.plot([0, 600], [0, 600], '--r') # Perfect prediction line\n",
    "plt.xlabel(\"Actual Insulin\")\n",
    "plt.ylabel(\"Predicted Insulin\")\n",
    "plt.title(\"Random Forest (New)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b1ab7-cd51-405e-b2df-c34e3889d11c",
   "metadata": {},
   "source": [
    "**Actually, from the results, we can see that the linear regression model performed better than the random forest. There could be instances where the model is overfiited especially for a small dataset like this one in which the model overfits and memorizes the noise rather than the real pattern. Hence, the linear regression is recommended.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79469114-d9c3-4637-8ddb-d4348132b219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
